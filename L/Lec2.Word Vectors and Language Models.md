(lec1没细看，大概就是讲了一下word2vec的实现方式就可以直接上手代码了)
# ppt

按照章节顺序意思如下

**炼金 (第2,3章)** -> **鉴宝 (第4章)** -> **提纯 (第5章)** -> **应用(造小道具) (第6章)** -> **升级(造神器) (第7章)**
## 1.Course Organization

不重要，继续

## 2.Review: Main idea of word2vec

复习word2vec

word2vec的实现就是给定一个由句子组成的列表，首先提取出所有的单词然后标记唯一化，接下来创建一个大小为n×n的矩阵，取一个值k，k的意思就是取多少附近的词，有的就+1。

然后使用一个工具（SVD，无需关注细节），将n×n的矩阵压缩为一个n×a的矩阵，a的意思就是保留几个维度的特征

## 2.Optimization: Gradient Descent

梯度下降。

梯度下降的意思就是选取正确的训练方向，核心是针对每一个训练的参数都进行计算，每次得到的值作为下一步选取的方向。

随机梯度下降的意思就是不取每一个值，而是随机取一部分参数，然后只针对这部分参数进行计算。

简单来说就是如果你是一个大厨，前者是每次加调料的时候把每一滴汤都喝过去才能知道要怎么加调料，后者是只喝几口就加调料。

因为其具有随机性，反而能跳出局部最优解的陷阱

Word2vec 的参数与计算有两个矩阵，一个是u一个是v。v的意思就是其作为中心词，u的意思就是其作为上下文的词。

Skip-gram的计算流程就是v点乘个u，得到的分数越高越好，然后再使用softmax函数变为0-1区间改为概率。

针对其有一个Bag of Words简化，它的意思就是不关心到底距离中心词多远，只用关心它有在距离中心词一定范围内。

Word2vec算法有两大家族——

其一是Skip-gram，其二是Continuous Bag of Words。前者采用softmax，后者采用Negative Sampling（负采样）

softmax的缺点就是每次计算概率的时候都得调用所有东西，资源开销非常大

Negative Sampling则是采用另一种架构，它不再预测哪个是对的单词，而是预测答案对不对，也就是说，只需要训练一个判断器，判断它到底是“好词”还是“坏词”

训练过程只需要给出几个好词和坏词（捏造配对），其他就让它自己判断是好还是坏。

所以最后的目标就是——

最大化模型认出真朋友的概率，最小模型认出假朋友的概率。

因此这里引出Sigmoid函数，这个函数将区间压缩至0-1，意思就是，输出接近1，那么就表示好，输出接近0，就表示差

综上，引出J_neg-sample函数，计算真朋友，假朋友，同时加上一个3/4 次方，稍微降低高频词被选中的概率，同时大大提高低频词被选中的概率。

这种方法的优点在于，计算量小，每次更新更新的内容很少。

## 3. Why not capture co-occurrence counts directly?

为什么不直接采用统计词频法（Count-Based）

主要是因为三个问题——

太长了 (High-dimensional)，如果我们的词汇表有50万个词，那每个词的词向量就有50万维！这太占地方了，简直是一场灾难。

太空了 (Sparsity)，在这50万维的向量里，绝大部分数字都是 `0`，因为一个词只会和少数词一起出现。这就像一个巨大的书架，上面只有零星几本书，大部分都是空的。

不结实 (Less Robust)，一个又长又空的向量，会让后续的机器学习模型很“困惑”，效果会大打折扣。

所以采用SVD瘦身，但直接瘦身效果不好，所以对计数值取对数，压制一下那些超高频词的影响，并且增加超高词频的上限，比如上限为100，当然也可以直接把这些功能词扔掉不要了。

再然后引出了GloVe，Word2vec 在玩预测，Count-Based 在数数再降维，而GloVe指出词与词之间的关系，隐藏在它们共现概率的“比率”之中

比如说 `ice` (冰) 和 `steam` (蒸汽)。

- 我们随便找一个词 `k`，比如 `solid` (固体)。`P(solid|ice) / P(solid|steam)` 这个比值会**非常大**

- 如果我们找另一个词 `k`，比如 `gas` (气体)。`P(gas|ice) / P(gas|steam)` 这个比值会**非常小**
- 如果我们找一个中性词 `k`，比如 `water` (水)。这个比值会**接近 1**

GloVe 的目标，就是让词向量的运算，直接去拟合这些“比率的对数”，这意味着，像 `king - man + woman` 这样的向量运算，能够直接捕捉到语义上的关系，因为它在数学上就和共现概率的比率挂钩了

所以，GloVe 巧妙地结合了两种方法的优点：

它像“基于计数”的方法一样，利用了全局的、丰富的统计信息。

它又像 Word2vec 一样，最终的目标是学习出高质量的低维词向量，并且训练过程非常快！

## 4. How to evaluate word vectors?

如何评估词向量好坏，主要给出两条路径

第一种评估方法是检测内部的，如果内部可以，那就可以，缺点是局部最优解。

第二种 是检测外部的，外部评价，让人类之类的评价，但是很主观，也无法得到很具体的信息。

第一种的实现方式可以是类比，就是简单的空间平移，来看看对不对。比如说男人之于祖父等同于女人之于x，x应该平移得到祖母。

第二种就是替换，和原本的旧模型进行对比，如果新的更好那就更好

## 5. Word senses and word sense ambiguity

在训练词向量的时候，有一个问题就是一个词可能会有多个意思，对于多个意思的词生成的词向量反而会不伦不类。比如说bank同时拥有银行和河岸的意思，最后向量就会在银行和河岸之间（当然bank在现实世界银行使用的次数更多，所以更偏向于银行，且强偏向于）。

早期的解决办法是生成一个bank1和bank2

现代的解决办法是发现可以把原本生成的高维词向量拆解成低维词向量，因为本质上就是线性叠加产生的结果

## 6.Deep Learning Classification: Named Entity Recognition (NER)

命名实体识别

NER 的任务，就是一个自动贴标签器，自动找出里面提出的单词来并且贴上对应的标签，比如说猫贴上宠物的标签等。

更深入的例子比如说

- `Last night, Paris Hilton wowed...` 这里的 `Paris Hilton` 是一个人名。
    
- `... in the Hilton Hotel in Paris ...` 这里的 `Hilton` 是酒店（组织）名的一部分，而 `Paris` 是一个地名。

模型可以正确分辨。

其作用是追踪热点，智能问答和舆情分析。

其简单制作思路是窗口分类器，一个词到底是不是“地名”，不能只看它自己，得看它的“邻居”。所以就是根据一个词和它周围邻居组成的“窗口”，来判断这个中心词的类别

简单来说，具体过程如下——

- 我们要判断句子 `the museums in Paris are amazing` 里的 `Paris` 是不是地名。
    
- 我们设定一个大小为 2 的窗口，以 `Paris` 为中心，抓住它和它的邻居们：`museums`, `in`, `Paris`, `are`, `amazing`。
    
- 我们去查炼金手册，得到这 5 个词各自的词向量（比如都是100维）。
    
- **最关键的一步：** 我们把这 5 个 100 维的向量，**头尾相连，拼接成一个超————长的 500 维大向量 `x_window`**！这个大向量，就浓缩了 `Paris` 和它周围环境的所有信息！
    
- 最后，我们把这个 500 维的“浓缩信息向量”，喂给我们接下来要制作的“分类魔法引擎”，让它来告诉我们答案

分类魔法引擎的核心就是神经网络。

- **原料入口 (`x`)**:
    
    - 这就是我们刚刚拼接好的那个 500 维的“窗口大向量” `x_window`。
        
- **第一道工序 (乘以 `W`)**:
    
    - `W` 是一个巨大的“转换矩阵”，你可以把它想象成一本厚厚的**“菜谱”**。这是需要我们去**学习**的东西！
        
    - `s = Wx`：这个步骤就是，用菜谱 `W`，对我们的原料 `x` 进行一次“烹饪”，把它从一个 500 维的大向量，“混合”成一个**单独的“分数” `s`**。这个分数就代表了模型初步判断“这个词是地名”的可能性有多大。
        
- **第二道工序 (非线性激活 `f`) :
    
    - 在真正的深度网络里，我们通常会在这里加一个 `f` 函数（比如 `tanh` 或 `ReLU`），给模型增加一些非线性，让它能学习更复杂的关系。就像做菜时加一点醋，能产生奇妙的化学反应。
        
- **出厂质检 (`σ`)**:
    
    - `σ(s)`：我们又见到老朋友 `Sigmoid` 函数啦！它就是那个“信心打分器”。
        
    - 它把我们得到的“分数” `s`，转化成一个 0 到 1 之间的、真正的**概率值**。比如 0.95，就表示“我有 95% 的把握，这绝对是个地名！”


**神经网络和传统方法的区别是什么？**

- 最大的区别就是：**我们不仅学习了菜谱 `W`，我们甚至可以同时微调（学习）词向量 `x` 本身**！我们让词向量为了更好地完成这个贴标签任务，自己去寻找更合适的位置。这就是深度学习强大的地方！

那么如何训练呢？

引出损失函数。

**交叉熵损失 (Cross Entropy Loss) 是什么？**

- 在二分类问题里，它其实就是**最小化正确答案的负对数概率**。
    
- `Loss = - log(p)` (这里的 `p` 是模型对**正确**答案的预测概率)

如果模型对正确答案的预测概率是 0.99 (非常有信心)，那 `Loss = -log(0.99)`，这是一个**非常接近 0 的小数字**，那么说明预测的很好

如果模型对正确答案的预测概率是 0.01 (错得离谱)，那 `Loss = -log(0.01)`，这是一个**非常大的数字**，那么说明预测的很差。

## 7. Neural computation

神经网络 

第一层：实现一个神经元

之前学的那个逻辑回归分类器，它其实就是神经网络世界里的一个**最最基本的单元**——一个**“神经元 (neuron)”**！

可以把它想象成一个“裁判”**：

- `x`: 是提交给它的**“证据”**（比如我们的窗口向量）。
    
- `w`: 是它脑子里的**“判案准则”**（权重），告诉它哪些证据更重要。
    
- `b`: 是它的**“固有偏见”**（偏置），就算没证据，它也可能倾向于某个判断。
    
- `w^T x + b`: 小裁判根据准则，把所有证据汇总，得出一个**“内心分数”**。
    
- `f`: 是 `Sigmoid` 函数，小裁判把内心分数，转化成一个标准的**“最终判决”**（0到1的概率）。

第二层：实现一个神经网络层

一个“神经网络层 (layer)，其实就是一堆神经元并排站在一起，每个神经元都有自己独特的“判案准则” `w` 和“固有偏见” `b`

无需关心训练细节，每个神经元自己训练成自己想要的，它们各自学会了从输入中提取某一种**特定的“特征”**

第三层~第N层：叠加神经网络层

第一层吃的是原始数据 ，第二层吃的是第一层生成的数据......

指导他们运行的是损失函数，损失函数直接指出第几层错了，然后去针对性修改，整个过程就是反向传播。

非线性函数 `f`

简单来说，如果没有非线性函数，只是线性函数训练无法提取更多特征，非线性函数的存在可以使距离很远的点变得很近（某种共性），更方便提取特征。

所以，一个深度神经网络就是把一张纸折叠出一只千纸鹤，无论原始数据（纸上的点）混得多么乱，我们都能通过复杂的折叠，最终用一把简单的“剪刀”（最后的分类器），轻松地把它们完美地分离开。

如果没有 `f` 这个“折叠”操作，就等于把同一张纸拉伸了100次，最后的效果和只拉伸1次没有任何区别。