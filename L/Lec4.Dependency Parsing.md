(反向传播在cs231n里被折磨过了，就不看lec3了)

然而虽然再也不想关心反向传播的具体原理，但是这里还是提及了一下，算是回顾。

前向传播Fprop就是信息输入x开始，一层层流过神经网络，最终得出一个输出结果，比如说loss

反向传播Bprop就是根据loss来反向定位出不合理的那一层，从而进行微调。

自动微分的意思就是说，在现代的代码实现，不需要关心具体的细节，只需要使用backward()就可以实现了。

（然而cs231n得纯numpy手搓，包含了大量的数学细节，苦不堪言）

forward和backward这两个函数如何使用？

只需要简单定义一个forward，告诉它怎么计算，然后用最终结果调用.backward就可以自动得出结论。

最后使用optimizer进行微调。

在没有pytorch的时代，需要手动实现前向传播和反向传播，但是很容易算错。

那么如何验算？就是故意把某种参数多加一些内容，然后看看loss最后的输出结果，通过loss的变化来观察这个参数的重要性。

当然有pytorch之后这些东西就不重要了。

然而，我们为什么要学习反向传播，为什么要理解反向传播？

很简单，因为只要理解了反向传播，就相当于理解了所有深度学习模型，因为反向传播是所有深度学习模型的基石，连接了所有深度学习模型的学习过程。

为了让你成为一个更好的魔法师，而不是一个仅仅会念咒语的机器！

比如未来某一天在训练更复杂模型的时候，会遇到梯度爆炸或者梯度消失的情况，如果你理解反向传播，那么久很容易的想到应该是在传递报告的过程中 ，数值变得太大了或者太小了，从而找到问题的解决方案。

我们即将要学习的东西，就是教你怎么实现作业2，分析句子。

（byd我写完了a2才开始学是吧🤬）

---

### 1. The linguistic structure of sentences – two views: Constituency = phrase structure grammar = context-free grammars (CFGs)

现在有两种构造句子的方式，第一种是`Constituency Parsing` (成分句法)，简而言之，就是把words拼接成模块phrases，比如说把the，cuddly，cat拼接成一个名词短语NP，the cuddly cat。

然后再把这些小模块和介词短语PP拼接在一起，比如说by the door，这样就可以组成一个更大的模块。

第二种方式是`Dependency Parsing` (依存句法)，简单来说就是一个句子就像一个大家庭，其中一定有一个核心动词，其他所有的单词都以某种关系直接或者简介地听从于这个动词。

首先先找到核心，其实很简单，比如说`Look in the large crate...` 这句话里，核心动作是 `Look`。它是整个家庭的“根”

其次画出关系线。

- `crate` 是 `in` 的“孩子”。
    
- `in` 又是 `Look` 的“孩子”。
    
- `the` 和 `large` 都是 `crate` 的“孩子”（修饰语）

最后得出一个树形结构，它不关心具体的模块，只关心”谁依赖谁“，”谁修饰谁“，”谁是核心“

---

为什么要使用这么麻烦地方式，原因有三。

其一，PP起义，意思就是介词短语有歧义。比如说现在有一个句子，`Scientists count whales from space.` (科学家从太空数鲸鱼)

那么这里有两种解释——

是“从太空”这个动作 (`count from space`) 吗？（科学家拿着太空望远镜在数）

还是这些鲸鱼是“来自太空的鲸鱼” (`whales from space`) 吗？

因此，一个介词短语可以有两种解释，从而产生完全不同的意思，这种可能性，还会随着句子地变长以指数级疯狂增长。

其二，并列歧义

现在有一个句子Shuttle veteran and longtime NASA executive Fred Gregory...

这里有两种解释——

是 `(Shuttle veteran)` 和 `(longtime NASA executive)` 这两个并列的身份，都属于 `Fred Gregory` 吗？（他既是老兵，又是高管）

还是说 `Fred Gregory` 这个人，他的身份是 `longtime NASA executive`，并且这个身份还和一个叫做 `Shuttle veteran` 的东西并列？（这就很奇怪了）

所以，and这个词到底连接多长的词块，决定了整个句子的意思。

其三，其他陷阱

**`Adjectival/Adverbial Modifier` (修饰歧义):** `southern food distributors` 到底是“南方的食品经销商”还是“南方食品的经销商”？

**`Verb Phrase attachment` (动词短语歧义):** `The board approved its acquisition by the company` 是“董事会批准了它被公司收购”还是“董事会被公司批准了它的收购”？

---

基于以上内容 ，为了破解以上的难题，所以要使用前面介绍的第二种实现方式，自动构建一个大型的树。

### 2. Dependency Grammar and Dependency Structure

依存语法蓝图的设计规范。

首先需要明确如何设计蓝图，句子里的词语关系不是平等的朋友关系，而是不对等的上下级关系，一定有一个领导和一个下属，且箭头永远从领导指向下属。

为了让关系图更清晰，不只是要画线，还要再线上贴标签，说明这是一个什么关系，比如说

- `nsubj:pass`: 你是我的“被动语态下的主语”。
    
- `obl`: 你是我的“间接宾语”。
    
- `appos`: 你是我的“同位语”，是用来解释我的。

整个流程必须是一张结构清晰的树，必须遵守以下原则

- 只有一个根，所有单词都直接或者简介地向根进行汇报
- 没有环
- 没有孤儿

---

这种实现方式其实比成分句法古老很多（了解一下就行了，直接贴ai翻译）

generate by gemini

这部分像是在给我们上历史课，告诉我们，我们现在用的这个方法，其实比那个“乐高积木法”（成分句法）要**古老得多**！

- **源远流长:** 它的思想，最早可以追溯到**公元前5世纪**古印度的语言学家**波你尼 (Pāṇini)**！比很多哲学思想的源头都要早！
    
- **全球开花:** 后来，它成为了阿拉伯、俄罗斯、中国等地区语言学家们的主流思想。因为它特别擅长分析那些词序比较自由、或者形态变化很丰富的语言（比如俄语、拉丁语）。
    
- **现代复兴:** 后来由一位叫做 **Tesnière** 的学者在1959年发扬光大，成为了我们现在遵循的标准。
    
- **一个有趣的小插曲:**
    
    - 箭头的方向，其实没有“宇宙真理”。有的人喜欢从“下属”指向“领导”，但我们现在遵循 Tesnière 的习惯，从**“领导”指向“下属”**。
        
    - 为了保证“人人有归属”这条铁律，我们通常会**虚构一个“ROOT”节点**，作为整个句子的“最高祖先”，这样所有的词就都有了唯一的最终归属。

---

那么构建整个树的单词来源用什么，用一个语料库，语料库是由人类语言学家一个个亲手标注好的标准关系图。

语料库有以下作用——

- **可重复利用的宝贵财富:** 无数人可以在上面训练和测试他们的模型。
    
- **语言学研究的基石:** 提供了海量的、真实的语言数据，而不只是几个例子。
    
- **评估标准:** 我们可以用它来给我们的模型打分，看看我们的模型盖的房子，和专家盖的到底有多像。

---

这部分有两个核心难点。

第一，信息来源。词语间具有亲和力，比如说issues这个词久很喜欢被discussion领导；大部分的领导和小鼠都在局子里距离很近，关系线很少跨越标点符号或动词，有些领导的词喜欢在左边加主语，右边什么都不带（反之亦然）

第二，`Non-projective` (非投射性) - “可以画交叉线吗？”

什么是投射性？

简而言之就是把句子里的词排成一排，所有的关系线都是从上方画出来的，且没有任何一条线交叉，大部分简单句都是这样的。

什么是非投射性，在某些语言的现象中 （比如英语里的长距离疑问句），为了表达正确意思，必须画出叫擦好的关系线。

比如说`Who did Bill buy the coffee from yesterday?`

`from` 的领导应该是 `Who`，因为我们问的是“从谁那里买”。但是 `Who` 在句首，`from` 在句尾，这条线必须跨过中间所有的词，形成一个巨大的交叉

这意味着在设计的时候必须要考虑到底允许画交叉线还是非交叉线。

---

### 3. Methods of Dependency Parsing

为了搭建起来，有四种主流的实现方式——

1. 动态规划，尝试所有可能的组合方式，然后用一个聪明的算法(O(n3))找出最好的方案。精确，但很慢
2. 图算法，将所有的词看成节点，找到一棵最小生成树
3. 约束满足，线列出一些禁止方案，如动词不能连接介词，然后一一排除
4. 转换分析法，只根据当前的情况，凭借直觉和经验迅速做一个贪心的决定。

---

显而易见，采用最后一种做法，整个流程有以下几个工具

第一，stack。一个后进先出的栈，将所有正在处理的单词放在这里。

第二，buffer，一个先进先出的队列，按照顺序放着所有还没处理的词

第三，A，记录每一个已经完成连接的操作。

整个流程只有三件事——

1. **`SHIFT` (抓取):** 从“传送带”(`Buffer`)的最前面，拿起一个词，然后放到“工作台”(`Stack`)的顶端。
    
2. **`LEFT-ARC` (左手连接):**
    
    - **条件：** 工作台上至少有两个词。
        
    - **动作：** 查看工作台最顶端的两个词 (`wi` 和 `wj`)。然后，建立一个从 `wj` 指向 `wi` 的连接（`wj → wi`），也就是让**右边的词当领导**。
        
    - **收尾：** 把 `wi` （那个当了“下属”的词）从工作台上**扔掉**，因为它已经找到归属了。
        
3. **`RIGHT-ARC` (右手连接):**
    
    - **条件：** 工作台上至少有两个词。
        
    - **动作：** 同样是查看最顶端的两个词。这次，建立一个从 `wi` 指向 `wj` 的连接（`wi → wj`），也就是让**左边的词当领导**。
        
    - **收尾：** 把 `wj` （那个当了“下属”的词）从工作台上**扔掉**。
        

讲义用 "I ate fish" 这个例子，完美地向我们展示了机器人是如何通过一系列 `SHIFT`, `LEFT-ARC`, `RIGHT-ARC` 操作，一步步地把一个句子分解成一棵完美的依存树的！

---

但是，每次遇到三个抉择，到底怎么知道下一步应该是做什么呢？

答案是机器学习。

在每一步都记录当前的状态。

比如说记录stack最前面的词是什么，词性是什么，比如说 记录buffer最前面的词是什么，词性如何等等。

将记录的内容喂给模型，模型输出一个打分（logits）告诉我们：

- “嗯... 根据我看到的这个情况，我认为下一步执行 `RIGHT-ARC` 的可能性是90%，`SHIFT` 是8%，`LEFT-ARC` 是2%。”
    
- **贪心的机器人：** 我们的机器人非常“贪心”，它会毫不犹豫地选择分数最高的那个动作去执行！

也可以不那么贪心，保留k个最有可能的施工方案 ，让k个机器人平行工作，最后选择最好的，慢，但是好。

---

那么如何评估呢，很简单，就是拿机器人画的关系图和语言学家设计的标准图对比。

**怎么算分？** “你的机器人总共画了100条关系线，其中有95条和标准图纸上画的‘领导’一模一样（标签可能不一样）。” 那么它的**无标签附着得分 (UAS)** 就是 95%。如果连“关系标签”也一样，那就是**有标签附着得分 (LAS)**。

---

### 4. Why do we gain from a neural dependency parser? Indicator Features Revisited

传统的解析器比如 `MaltParser`，虽然很快，但是有以下问题——

第一，稀疏

第二，不完整

第三，每次决策前都要花大量的时间查找（如95%的时间）

---

将传统的解析器更换为神经网络，有以下两个优点。

第一，分布式表示，其实就是得到浓缩的词向量。

每一个词、每一个词性(POS)、甚至每一个依存关系标签，都是一个。

意思相近的词在图上很近。

在作业里做的事情就是从对应位置提取特征，然后和对应的词向量拼接。

第二，非线性分类。

传统的分类器是一把直尺，只能在数据中间画一条直线分类。

由于ReLU等的存在，神经网络的切线可以任意弯曲。

---

这部分不重要。

generate by gemini

- **惊人的结果:** 讲义里的那张表格告诉了我们这场革命有多成功！
    
    - 陈和曼宁的模型 (C & M 2014)，在**准确率 (UAS/LAS) 上**，几乎追平了当时最精确但慢得像乌龟的 `TurboParser`。
        
    - 但在**速度 (sent./s) 上**，它又比当时最快的 `MaltParser` 还要**快上一大截**！
        
    - **它同时实现了“又快又准”！这是一个历史性的突破！**
        
- **继承与发展:**
    
    - 后来，谷歌基于这个思想，把网络做得更大、更深，还加入了“beam search”（不那么贪心了），最终做出了大名鼎鼎的 **SyntaxNet (Parsey McParseFace)**，成为了当时的世界第一！
        
- **另一条技术路线：图分析法 (Graph-based):**
    
    - 最后，讲义还提到了我们的老朋友——那个“电气工程师”流派。
        
    - 在神经网络时代，这个流派也复活了！Dozat 和曼宁 (2017) 设计了一个聪明的模型，它不再一步一步做决策，而是一口气计算出**所有词之间**可能存在的**所有连接**的分数，然后用算法找出最优的那棵“树”。
        
    - 这种方法**更准**（因为它看得更全面），但通常也**更慢**（因为它要计算 `n²` 数量级的连接）