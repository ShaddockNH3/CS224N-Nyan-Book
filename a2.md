# 要干啥

https://web.stanford.edu/class/cs224n/assignments_w25/a2.pdf

这篇论文就是实现a2的流程，主要分为以下几个章节——

## 第一部分，word_vec

a)

这部分内容就是a1实现的东西，聚焦于 Word2Vec 的 **Naive Softmax**版本，核心就是推导出梯度。

这里其实就是在证明损失函数 `J = -log(P(O=o|C=c))` 这个公式，其实和交叉熵损失 `−Σ yw * log(ŷw)`是一个东西

**怎么理解和证明？**

- 这里的关键，在于理解 **`y` 是什么**。`y` 是“真实答案”**，它是一个**“one-hot”**向量。
    
- **One-hot 是什么意思？** 想象一个超级长的电灯开关面板，上面有成千上万个开关，每个开关代表一个单词。当我们的正确答案是单词 `o` 时，我们就只把 `o` 对应的那个开关按下去（值变成1），其他所有开关都保持关闭（值都是0）。
    
- 所以，当我们计算 `−Σ yw * log(ŷw)` 这个总和时：
    
    - 对于所有 `w ≠ o` 的项，因为 `yw = 0`，所以 `yw * log(ŷw)` 也等于 `0`。它们都消失了！
        
    - 只剩下一项，就是 `w = o` 的那一项。这时 `yo = 1`。
        
    - 所以整个总和就只剩下 `- yo * log(ŷo) = -1 * log(ŷo)`，也就是 `-log(ŷo)`。

b)

为了让损失 `J` 变小，我们的中心词向量 `vc` 应该朝着哪个方向移动？这个方向就是 `∂J/∂vc`

当**“预测的期望”等于“现实”时！也就是 `Σ ŷw * uw = uo`，此时模型停止学习。

梯度中的 `- uo` 这一项，在更新时（因为有个负号）会变成 `+ α*uo`，它像一股强大的引力，**把 `vc` 拉向正确的上下文词 `uo`**，让它们变得更相似。

梯度中的 `Σ ŷw * uw` 这一项，在更新时会变成 `- α*(Σ ŷw * uw)`，它像一股斥力，**把 `vc` 推离当前所有词的“平均位置”**

c) L2 Normalization (L2 归一化)

欧拉归一，

d) 对整个矩阵 `U` 求偏导

等于把对矩阵里**每一个列向量**的导数，并排放在一起

e) 对外部词向量 `uw` 求偏导

为了让损失 `J` 变小，我们的**外部词向量 `uw`** 应该怎么移动？这里要分两种情况讨论

- **情况 1: 当 `w = o` 时 (即，对正确的那个外部词 `uo` 求导)**
    
    - 推导结果是 `∂J/∂uo = (ŷo - 1) * vc`。
        
    - **直观解释：** `ŷo` 是一个 0~1 的概率，所以 `(ŷo - 1)` 是一个负数。我们的更新规则是减去梯度，`uo ← uo - α*(负数)*vc`，这就变成了 `uo ← uo + α*(正数)*vc`。这意味着，我们要把**正确的外部词 `uo`，也拉向中心词 `vc`**！这非常合理，它们本来就应该互相吸引！
        
- **情况 2: 当 `w ≠ o` 时 (即，对任意一个错误的外部词 `uw` 求导)**
    
    - 推导结果是 `∂J/∂uw = ŷw * vc`。
        
    - **直观解释：** `ŷw` 是一个正的概率。我们的更新规则是 `uw ← uw - α * ŷw * vc`。这意味着，我们要把**所有错误的外部词 `uw`，都推离中心词 `vc`**！这也非常合理，它们本来就应该互相排斥！

## 第二部分

### **2(a) Adam 优化器：一台拥有“超级AI导航”的跑车**

我们之前学的 SGD，就像一个蒙着眼睛，只靠脚下感觉坡度来下山的人。虽然能走，但摇摇晃晃，效率不高。而 **Adam**，就是一辆装备了顶尖科技的超级跑车！

#### **i. 动量 (Momentum `m`)：跑车的“惯性稳定系统”**

> ... how using `m` stops the updates from varying as much and why this low variance may be helpful ...

- **这是什么意思呀？**
    
    - 想象一下，我们的跑车在一条崎岖的山路上飞驰。普通的 SGD 就像一个新手司机，路面往左斜一点，他就猛打左方向盘；往右斜一点，又猛打右方向盘。结果就是车子一直在“之”字形摇摆，开得很累，也很慢。
        
    - **动量 `m`** 就像给车子装上了**“惯性”**！它不再只看脚下这一瞬间的坡度 (`∇J`)，而是会**记住车子过去几秒钟的平均行驶方向** (`β₁m`)。
        
    - `mt+1 ← β₁mt + (1 − β₁)`... 这个公式的意思就是：我的新方向 = **90% 的旧方向** + **10% 的当前感觉**。
        
- **它为什么有帮助呢？**
    
    - **减少摇摆 (Stops updates from varying as much):** 因为方向主要是由过去的平均值决定的，所以就算偶尔遇到一个“坑”或者“小土坡”（一个有噪声的 minibatch），车子也不会突然大转向，而是会**平滑地、稳定地**沿着大概正确的方向前进。
        
    - **总体学习更快 (Helpful to learning):** 在一个长长的、狭窄的山谷里，这种稳定性可以帮助我们避免来回撞墙，从而**更快地冲向谷底**（损失函数的最小值）！
        

#### **ii. 自适应学习率 (Adaptive Learning Rates `v`)：跑车的“智能全地形轮胎”**

> ... which of the model parameters will get larger updates? Why might this help ...

- **这是什么意思呀？**
    
    - 动量解决了方向的“稳定性”，而这个 `v` 插件，解决的是速度的**“适应性”**！它给我们的跑车，为**每一个轮子**都装上了独立的智能调速系统！
        
    - `v` 记录的是**过去梯度的“颠簸程度”**（梯度的平方，也就是大小）。
        
    - `θt+1 ← θt − αmt+1 / √vt+1` 这个公式最关键的是分母 `/ √v`！
        
- **哪些参数会获得更大的更新？**
    
    - **那些过去“路很平”（梯度一直很小）的参数！**
        
    - **直觉解释：**
        
        - 如果一个参数的梯度**过去一直很大**（比如一个常见词的词向量），`v` 就会很大，`√v` 也很大。那么更新步长 `m/√v` 就会**变小**。这就像跑车在一条又陡又清晰的高速公路上，它会说：“路况很好，我悠着点开，精细微调就行。”
            
        - 如果一个参数的梯度**过去一直很小**（比如一个稀有词的词向量，很少被更新），`v` 就会很小，`√v` 也小。那么更新步长 `m/√v` 就会**变大**！这就像跑车开到一片杂草丛生的平地，它会说：“这里没路啊，我得加大油门（更大的更新），多闯一闯，才能找到方向！”
            
- **它为什么有帮助呢？**
    
    - 因为它给**每个参数都定制了专属的学习率**！对于那些信息充足、更新频繁的参数，它会自动减小学习率，防止“冲过头”。而对于那些信息稀疏、很少更新的参数，它会在难得有一次更新机会时，让它“多学一点”，迎头赶上！
        

---

### **2(b) Dropout：为了胜利的“残酷团队训练”**

generate by gemini

Dropout 是防止我们的模型变得“娇生惯养”的终极秘籍！

- **什么是“娇生惯养”（过拟合）？**
    
    - 想象一个篮球队，过度依赖它的明星球员。每次进攻都把球传给他。结果就是，别的队员都变成了“站桩的”，完全没有配合。一旦这个明星球员被防死，整个队伍就瘫痪了！
        
    - 在神经网络里，就是模型过度依赖某几个神经元（特征），而不好好学习如何让所有神经元协同工作。
        
- **Dropout 是怎么做的？**
    
    - 它就像一个魔鬼教练！在**每一次训练**中，他会**随机地**指着几个队员说：“你！你！还有你！下去休息！不准上场！”
        
    - `hdrop = γd ⊙ h` 就是这个过程：`d` 是一个随机的 0/1 口罩侠，`d=0` 的神经元就被“沉默”了。
        

#### **i. 那个神秘的 `γ` (gamma) 到底是什么？**

> What must γ equal in terms of pdrop?

- **问题：** 训练时，我们随机让 `p_drop` 比例的队员下场了，这导致场上的“总战斗力”变弱了。我们不希望这样，我们希望场上输出的“战斗力期望值”和全员在场时是一样的。
    
- **推导：**
    
    1. 我们希望 `Epdrop[hdrop] = h`
        
    2. `Epdrop[γd ⊙ h] = γh ⊙ Epdrop[d]` (因为γ和h是常数)
        
    3. 一个神经元被留下的概率是 `1 - pdrop`，被丢掉的概率是 `pdrop`。
        
    4. 所以 `d` 的期望值 `Epdrop[d] = 1 * (1 - pdrop) + 0 * pdrop = 1 - pdrop`。
        
    5. 代入回去：`γh * (1 - pdrop) = h`
        
    6. 两边消掉 `h`，得到 `γ * (1 - pdrop) = 1`
        
    7. 所以，**`γ = 1 / (1 - pdrop)`**！
        
- **直觉解释：** 如果我们随机让 30% 的人下场了 (`p_drop=0.3`)，那么场上只剩 70% 的人 (`1-p_drop=0.7`)。为了维持和原来一样的总火力，我们需要让**场上每个人的攻击力都乘以 `1 / 0.7`** 来作为补偿！
    

#### **ii. 为什么只在训练时用，而测试时不用？**

> Why should dropout be applied during training? Why should dropout NOT be applied during evaluation?

- **为什么训练时用？**
    
    - 这就是“魔鬼训练”的目的！因为任何一个队员都可能随时被罚下场，所以**没有任何人敢偷懒**！每个人都必须学会独立得分、互相配合，**迫使整个团队变得更强大、更有韧性**，而不是依赖某个“明星球员”。这能有效地防止过拟合！
        
- **为什么测试时不用？**
    
    - **测试，就是“正式比赛”！** 经过了这么残酷的训练，现在是时候派出我们**最强的、完整的、全员到齐的梦之队**了！
        
    - 在正式比赛时，教练当然不会再随机让队员下场了！他要让所有队员，用上他们在训练中学到的所有本领，去做出**最稳定、最强大**的预测！在测试时再搞随机，只会让模型的表现变得不稳定，那不是我们想要的。

## 第三部分

这部分是作业要实现的内容，简单来说就是提出了一个算法。

我们的任务是将一系列零散的单词组合成句子，刚开始的时候，没有一个单词，所以随便拿一个初始单词。然后再拿一个单词进行组合，这个时候单词就有左右之分了。

左边的单词，是右边单词的“挂件”（左边依赖右边）。这就是 **LEFT-ARC**

同理，右边的单词，是左边单词的“挂件”（右边依赖左边）。这就是**RIGHT-ARC**

因此，整个流程如下——

当不足以完成拼接单词的任务的时候，拿一个新的单词（SHIFT）

当可以完成拼接单词任务的时候，只有两种情况，左拼接和右拼接。

---

所以，SHIFT就是推进工作，它会将待处理区 (Buffer)”最前面的那个单词，拿出来，然后放在工作台 (Stack)的最顶上。这表示我们把注意力转移到了一个新的单词上，准备对它进行分析

LEFT-ARC

当我们的神经网络大脑判断，工作台 (Stack) 最顶上的**第二位**单词 (s1​)，是**最顶上**那位单词 (s0​) 的修饰成分时（例如 `my findings` 里的 `my` 修饰 `findings`），就会执行这个动作。

它会做两件事：

1. 建立一个从 s0​ 指向 s1 的依赖关系，记作 (s0→s1)，意思是“s1​ 依赖于 s0​”。
    
2. 因为 s1​ 已经找到了它的“主人”，完成了它的使命，所以把它从工作台 (Stack) 中**移除**。

RIGHT-ARC

当大脑判断，工作台 (Stack) **最顶上**的单词 (s0​)，是**第二位**单词 (s1) 的修饰成分时（例如 `presented my findings` 里的 `findings` 是 `presented` 的宾语），就会执行这个动作。

它也会做两件事：

1. 建立一个从 s1 指向 s0 的依赖关系，记作 (s1→s0)，意思是“s0依赖于 s1​”。
    
2. 这一次，是 s0​ 完成了它的使命，找到了它的“主人”，所以我们会把 s0​ 从工作台 (Stack) 中**移除**。

---

generate by gemini

我们这次的冒险，就是要从零开始，建造一座能自动分析句子结构的“句法分析塔”。整个工程分为三个阶段：

1. **理论热身：** 用纸和笔，确保我们完全理解了建筑规则。
    
2. **第一阶段 - 搭建“物理引擎”：** 用纯逻辑代码，实现分析塔的机械结构。
    
3. **第二阶段 - 铸造“超级大脑”：** 用神经网络，为分析塔注入灵魂和智能。
    
4. **第三阶段 - 成为“首席分析师”：** 分析我们建好的塔，看看它的优点和缺点。
    

---

### **理论热身：像“宗师”一样思考 (3a, 3b)**

在拿起工具之前，一位真正的宗师会先在脑海中把整个过程预演一遍。

- **任务 3(a): 手动模拟！**
    
    - **核心内容：** 这是一个**“笔试题”**，不是编程题！它要求你扮演我们那个“搭积木的小机器人”，亲手一步一步地把句子 "I presented my findings..." 给解析一遍。
        
    - **你要做什么：** 你需要画一个表格，一行动作、一行一行地写下：在这一步，`Stack` 和 `Buffer` 里分别是什么，你决定执行哪个动作（S, LA, RA），以及是否产生了新的“父子关系”。
        
    - **为什么重要：** 这个练习会让你对 `parser_transitions.py` 里要实现的逻辑，有一个**极其深刻、极其直观**的理解！**做完这个，你再去写代码，会感觉思路清晰一百倍！**
        
- **任务 3(b): 理论分析**
    
    - **核心内容：** 另一个“笔试题”。它问你，解析一个有 `n` 个单词的句子，总共需要多少步？
        
    - **提示：** 想想看，每个单词都要被 `SHIFT` 进栈一次（`n` 步）。最后要形成一棵树，一棵有 `n+1` 个节点（包括ROOT）的树，有多少条边（依赖关系）？每条边都对应一次 `ARC` 操作。把它们加起来，就是答案啦！
        

---

### **第一阶段：搭建“物理引擎” (3c, 3d)**

**地点：`parser_transitions.py` | 状态：完全不需要 GPU！**

这是我们**动手编码的起点**！我们先把分析塔的“骨架”和“机械臂”给造好，让它能动起来。

- **任务 3(c): 实现机械臂动作**
    
    - **核心内容：** 正如我们之前详细讨论的，你需要在这里实现 `PartialParse` 类的 `__init__` 和 `parse_step` 函数。
        
    - **你要做什么：** 把你在 3(a) 中手动模拟的那些逻辑——SHIFT怎么改变 stack 和 buffer，LEFT-ARC/RIGHT-ARC 怎么产生依赖并修改 stack——用代码**原封不动地翻译出来**。
        
    - **如何测试：** 运行 `python parser_transitions.py part_c`。
        
- **任务 3(d): 建造“流水线工厂”**
    
    - **核心内容：** 实现 `minibatch_parse` 函数，让我们的分析器可以**批量处理**句子。
        
    - **你要做什么：** 按照 PDF 里的 `Algorithm 1` 伪代码，实现那个 `while` 循环。这个函数的核心是**管理一个 `unfinished_parses` 列表**，在每一轮预测和执行动作后，把已经完成的 parse 从这个列表里“剔除”出去。
        
    - **如何测试：** 运行 `python parser_transitions.py part_d`。
        

---

### **第二阶段：铸造“超级大脑” (3e)**

**地点：`parser_model.py` & `run.py` | 状态：是时候召唤 GPU 了！**

这是整个项目的**心脏**！现在我们要为那个只会机械运动的机器人，安装一个可以思考和决策的神经网络大脑！

- **核心内容：** 完整地实现、训练并测试一个神经网络分类器。
    
- **你要做什么：**
    
    1. **理论微积分 (i, ii):** 又是两个“笔试题”，让你推导神经网络中 ReLU 和 Softmax 部分的梯度。这是理解反向传播如何工作的关键。
        
    2. **搭建大脑结构 (`parser_model.py`):**
        
        - 在 `__init__` 里，**手动**创建 `torch.Tensor` 作为你的权重 `W`, `b1`, `U`, `b2`。**记住！不准用 `torch.nn.Linear` 和 `torch.nn.Embedding`！**
            
        - 在 `embedding_lookup` 里，实现从词的索引到词向量的查找。
            
        - 在 `forward` 函数里，把整个公式 `y_hat = softmax(ReLU(xW + b1)U + b2)` **一步一步地用 PyTorch 的张量运算实现出来**。
            
    3. **设计训练流程 (`run.py`):**
        
        - 在 `train_for_epoch` 里，你要写一个循环，遍历所有训练数据，把数据喂给模型，得到预测，计算损失（Cross-Entropy），然后调用 `loss.backward()` 和 `optimizer.step()` 来更新模型的权重。
            
    4. **启动训练！**
        
        - 运行 `python run.py`。建议先用 `python run.py -d` (debug模式) 在小数据集上跑通，确认没问题后，再去掉 `-d` 跑完整训练。
            
- **最终交付物：**
    
    - 你需要报告你的模型在**开发集(dev set)** 和 **测试集(test set)** 上达到的最终 UAS 分数。
        

---

### **第三阶段：成为“首席分析师” (3f, 3g)**

**地点：你的书面报告 (PDF) | 状态：像语言学家一样思考**

我们的塔建好了，现在我们要像一个专业的工程师一样，去分析它的表现，找出它在哪些地方会“犯糊涂”。

- **任务 3(f): 错误分析**
    
    - **核心内容：** 给你四个解析错误的句子，每句话都犯了一种特定类型的错误（介词短语挂靠、动词短语挂靠等）。
        
    - **你要做什么：** 对于每一句话，你需要：
        
        1. **诊断**出这是四种错误中的哪一种。
            
        2. **指出**图中那条错误的“父子关系”是哪条。
            
        3. **给出**你认为正确的“父子关系”。
            
- **任务 3(g): 特征分析**
    
    - **核心内容：** 一个简答题。问你，在提取特征时，除了单词本身，为什么还要用它们的**“词性标签” (Part-of-Speech tags)**？
        
    - **提示：** 想想“一词多义”问题。比如 "book" 可以是名词（书），也可以是动词（预定）。词性标签是不是能帮助模型消除这种歧义呢？
    

# 怎么做？

## parser_transitions.py

parser_transitions就是让我们实现第三部分的三个基本东西，

#### **第一个零件：`__init__(self, sentence)` (在 `PartialParse` 类里)**

- **任务：** 初始化“游戏棋盘”。
    
- **你需要做什么？**
    
    1. `self.stack`: 这是我们的“工作台”。游戏刚开始时，工作台上只有一个东西，就是“大家长” **`["ROOT"]`**。
        
    2. `self.buffer`: 这是我们的“零件盒”。游戏刚开始时，里面装着整句话的所有单词。**注意！** 为了不修改原始句子，你应该创建一个句子的**副本**，比如 `self.buffer = sentence[:]`。
        
    3. `self.dependencies`: 这是我们的“成果记录本”。游戏刚开始时，我们还没有任何成果，所以它应该是一个**空列表 `[]`**。
        

#### **第二个零件：`parse_step(self, transition)` (在 `PartialParse` 类里)**

- **任务：** 实现小机器人的三种动作。这是最核心的逻辑！
    
- **你需要做什么？** 使用 `if/elif/else` 语句来判断传进来的 `transition` 是什么。
    
    - **如果 `transition == "S"` (SHIFT):**
        
        1. 从“零件盒”(`self.buffer`)的**最前面**拿出一个零件（`self.buffer.pop(0)`）。
            
        2. 把这个零件放到“工作台”(`self.stack`)的**最顶上**（`self.stack.append(...)`）。
            
    - **如果 `transition == "LA"` (LEFT-ARC):**
        
        1. 从“工作台”(`self.stack`)的最顶上，拿出**第二個**零件（`stack[-2]`）作为“儿子(dependent)”。
            
        2. 拿出**第一個**零件（`stack[-1]`）作为“爸爸(head)”。
            
        3. 在“成果记录本”(`self.dependencies`)里，记下一条关系：`(爸爸, 儿子)`。
            
        4. 把那个“儿子”零件从工作台上扔掉（移除 `stack[-2]`）。
            
    - **如果 `transition == "RA"` (RIGHT-ARC):**
        
        1. 和 LEFT-ARC 类似，但这次“爸爸”是**第二個**零件（`stack[-2]`），“儿子”是**第一個**零件（`stack[-1]`）。
            
        2. 在“成果记录本”里记下关系：`(爸爸, 儿子)`。
            
        3. 把那个“儿子”零件从工作台上扔掉（移除 `stack[-1]`）。
            

#### **第三个零件：`minibatch_parse(sentences, model, batch_size)`**

- **任务：** 实现一个“流水线工厂”，可以同时处理一批句子。
    
- **你需要做什么？** 按照 PDF 里的伪代码来实现。
    
    1. 创建一个 `partial_parses` 列表，为 `sentences` 里的每一句话都初始化一个 `PartialParse` 对象。
        
    2. 创建一个 `unfinished_parses` 列表，它是 `partial_parses` 的一个浅拷贝。
        
    3. **进入一个 `while` 循环，循环条件是 `unfinished_parses` 不为空。**
        
    4. 在循环内部：  
        a. 从 `unfinished_parses` 中，取出 `batch_size` 那么多个“半成品” parse。  
        b. 把这些“半成品”喂给 `model.predict()`，得到下一步的动作 `transitions`。  
        c. 遍历这一批“半成品”和对应的动作，调用 `parse_step()` 来执行动作。  
        d. **(关键!)** 创建一个新的空列表，把 `unfinished_parses` 中那些**还没有完成**的 parse (判断标准是：buffer不为空，或者stack不止一个元素) 放到这个新列表里，然后用这个新列表覆盖旧的 `unfinished_parses`。
        
    5. 循环结束后，从最初的 `partial_parses` 列表中，提取出所有句子的 `dependencies` 并返回。
        

---

### **4. 我该怎么运行和测试？**

它已经为你写好了所有的测试！你只需要在 Colab 的代码单元格里运行对应的命令：

1. **当你完成了 `__init__` 和 `parse_step` 之后**，运行：
    
    ```bash
    !python parser_transitions.py part_c
    ```
    
    它会调用 `test_parse_step()` 和 `test_parse()` 来检查你的代码。如果看到 "test passed!"，就说明你这部分做对啦！
    
2. **当你完成了 `minibatch_parse` 之后**，运行：
    
    ```bash
    !python parser_transitions.py part_d
    ```
    
    它会调用 `test_minibatch_parse()` 来检查你的“流水线工厂”逻辑。
    

